# This YAML file represents the project configuration for the NLP project.
# It contains settings and parameters that are used to configure and run the project.
#
title: "CRVCIOIS SNI Stacked Ensemble AI Model pipeline"
description: "This is a training and evaluation pipeline for a Stacked Ensemble AI model which predicts a companys SNI code based on their website data."
# Variables can be referenced across the project.yml using ${vars.var_name}
# Filename, as 'train' only specifies the actual name and not where it's stored or it's extension
vars:
    # General settings
    config: "config_ensemble"
    version: "0.0.2"
    train: "docs_nace_training"
    dev: "docs_nace_eval"
    test: "docs_nace_test"
    gpu_id: -1
    # SCB settings
    scb_data_file_name: "scb_data"
    # Google API settings
    google_limit: 100
    regenerate_company_urls: False
    # Scraping settings
    scraped_data_folder: "scraped_data"
    follow_links: False
    scrape_filter: True
    # Extract settings
    extract_meta: True
    extract_body: True
    extract_p_only: False
    # Divide settings
    percentage_training_split: 70
    percentage_validation_split: 20
    percentage_test_split: 10
    # Preprocess settings
    min_data_length: 150
    # Evaluate and prediction settings
    model_to_evaluate: "training/model-best"
    evaluate_top_n: 5
    predict_url: "https://www.rh-markiser.se/"

# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories: ["assets", "training", "configs", "scripts", "corpus", "${vars.scraped_data_folder}", "logs"]

# Assets that should be downloaded or available in the directory. We're shipping
# them with the project, so they won't have to be downloaded. But the
# 'project assets' command still lets you verify that the checksums match.

# assets:
#   - dest: "assets/${vars.train}.json"
#     description: "JSONL-formatted training data exported"
#   - dest: "assets/${vars.dev}.json"
#     description: "JSONL-formatted development data)"

# Workflows are sequences of commands (see below) executed in order. You can
# run them via "spacy project run [workflow]". If a commands's inputs/outputs
# haven't changed, it won't be re-run.
workflows:
    all:
        - SCB
        - google
        - scrape
        - extract
        - divide
        - preprocess
        - train-model
    
    fetch:
        - SCB
        - google
        - scrape
        - extract



    train-evaluate:
        - train-model
        - evaluate-accuracy
        - evaluate-custom

    evaluate:
        - evaluate-accuracy
        - evaluate-custom

# Project commands, specified in a style similar to CI config files (e.g. Azure
# pipelines). The name is the command name that lets you trigger the command
# via "spacy project run [command] [path]". The help message is optional and
# shown when executing "spacy project run [optional command] [path] --help".
commands:
    - name: "SCB"
      help: "Get data from SCB"
      script:
          - "python pipeline/scb.py"

    - name: "google"
      help: "Fill the DB with a matching URL for each company by using Google search API"
      script:
          - "python pipeline/_google.py ${vars.regenerate_company_urls} ${vars.google_limit}"

    - name: "scrape"
      help: "Scrapes websites"
      script:
          - "python pipeline/scrape.py ${vars.scraped_data_folder} ${vars.follow_links} ${vars.scrape_filter}"

    - name: "extract"
      help: "Extracts the valuable data from the scraped website"
      script:
          - "python pipeline/extract.py ${vars.scraped_data_folder} ${vars.extract_meta} ${vars.extract_body} ${vars.extract_p_only}"
      deps:
          - "${vars.scraped_data_folder}"

    - name: "divide"
      help: "Divides the dataset into training and validation sets"
      script:
          - "python pipeline/divide_dataset.py  ${vars.percentage_training_split} ${vars.percentage_validation_split} ${vars.percentage_test_split}"

    - name: "preprocess"
      help: "Convert the data to spaCy's binary format"
      script:
          - "python pipeline/preprocess.py corpus/${vars.train}.spacy corpus/${vars.dev}.spacy corpus/${vars.test}.spacy ${vars.min_data_length}"

    - name: "train-model"
      help: "Train a text classification model"
      script:
          - "python -m spacy train configs/${vars.config}.cfg --output training/ --paths.train corpus/${vars.train}.spacy --paths.dev corpus/${vars.dev}.spacy --gpu-id ${vars.gpu_id}"
      deps:
          - "corpus/${vars.train}.spacy"
          - "corpus/${vars.dev}.spacy"
          - "configs/${vars.config}.cfg"
      outputs:
          - "training/model-best"

    - name: "evaluate-accuracy"
      help: "Evaluate the prod model for accuracy and export metrics"
      script:
          - "python -m spacy benchmark accuracy training/model-best corpus/${vars.test}.spacy --output training/metrics-accuracy-${vars.test}.json --gpu-id ${vars.gpu_id}"
      deps:
          - "corpus/${vars.dev}.spacy"
          - "training/model-best"
      outputs:
          - "training/metrics-accuracy-${vars.test}.json"

    - name: "predict"
      help: "predict the SNI code of a company based on their website data"
      script:
          - "python pipeline/predict.py  ${vars.model_to_evaluate} ${vars.predict_url}"

    - name: "evaluate-custom"
      help: "Custom evaluation of the model"
      script:
          - "python pipeline/evaluate.py ${vars.model_to_evaluate} ${vars.min_data_length} ${vars.evaluate_top_n}"
